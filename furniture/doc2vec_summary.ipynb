{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explicit Semantic Analysis\n",
    "\n",
    "Build a semantic interpreter that maps fragments of natural language text into a weighted sequence of Wikipedia concepts ordered by their\n",
    "relevance to the input. Input texts are represented\n",
    "as weighted vectors of concepts, called interpretation vectors.\n",
    "The meaning of a text fragment is thus interpreted in terms\n",
    "of its affinity with a host of Wikipedia concepts\n",
    "\n",
    "+ Represent each Wiki article (concept) using tfidf\n",
    "+ ? Build inverted index, which maps each word into a list of concepts in which it appears\n",
    "+ Represent each text using tfidf\n",
    "+ Represent each text using wiki concept, with weight $\\sum_{w_{i}\\in{T}}v_{i}k_{ij}$, where $v_{i}$ denotes tfidf vector, $k_{ij}$ denotes inverted index \n",
    "+ Calculate similarity using cosine distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dense Cohort of Terms (dCoT)\n",
    "\n",
    "Maps high-dimensional (overly) sparse vectors into a low-dimensional dense representation.\n",
    "+ Corruption (training of W is based on: If a prototype\n",
    "term already exists in some input x, W should be\n",
    "able to predict it from the remaining terms in x)\n",
    "+ Reconstruction (essentially a least square estimation for mapping function)\n",
    "+ Recursive re-application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BM25\n",
    "\n",
    "BM25 weighting scheme follows a probabilistic approach to assign a document with a ranking score given a query. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. doc2vec\n",
    "\n",
    "Learn word vector representation. Documents are composed of words, the similarity between words can be used to create a similarity measure between documents. \n",
    "\n",
    "+ Centroid of the word vectors: mean of its words vectors (baseline)\n",
    "+ Word Moverâ€™s Distance (WMD) : some variants; low bound is centroid of the word vectors (Kusner 2015; try gensim)\n",
    "+ doc2vec http://rare-technologies.com/doc2vec-tutorial/ (le & Mikolov 2014; try gensim)\n",
    "+ (Arora 2017)\n",
    "\n",
    "+ other: \n",
    "\n",
    "2011 - unfolding recursive autoencoder (very comparatively simple. start here if interested)\n",
    "\n",
    "2012 - matrix-vector neural network\n",
    "\n",
    "2013(?) - neural tensor network\n",
    "\n",
    "2015 - Tree LSTM\n",
    "\n",
    "his papers are all available at socher.org. Some of these models are available, but I'd still recommend gensim's doc2vec. For one, the 2011 URAE isn't particularly powerful. In addition, it comes pretrained with weights suited for paraphrasing news-y data. The code he provides does not allow you to retrain the network. You also can't swap in different word vectors, so you're stuck with 2011's pre-word2vec embeddings from Turian. These vectors are certainly not on the level of word2vec's or GloVe's.\n",
    "\n",
    "Haven't worked with the Tree LSTM yet, but it seems very promising!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
