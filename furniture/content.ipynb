{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read all furniture jsons\n",
    "data = []\n",
    "for file_name in glob('../../desktop/data/*.json'):\n",
    "    with open(file_name) as f:\n",
    "        temp = json.load(f)\n",
    "        for sku in temp:\n",
    "            # read parent product list and product list\n",
    "            parent_prod = list(sku['elasticsearch_result']['parentProducts'].keys())\n",
    "            prod = list(sku['elasticsearch_result']['products'].keys())\n",
    "            image = sku['main_image']['raw']\n",
    "            sku['image_url'] = image\n",
    "            if parent_prod:\n",
    "                sku['parentProducts'] = parent_prod[0]\n",
    "                if prod[0] == parent_prod[0]:\n",
    "                    sku['products'] = prod[1]\n",
    "                else:\n",
    "                    sku['products'] = prod[0]\n",
    "            else:\n",
    "                sku['parentProducts'] = np.nan\n",
    "                if prod:\n",
    "                    sku['products'] = prod[0]\n",
    "                else:\n",
    "                    sku['products'] = np.nan\n",
    "        data = data + temp\n",
    "\n",
    "\n",
    "# convert list into dataframe\n",
    "raw_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# select key features\n",
    "# features = ['title', 'category_id', 'category_level_0', 'category_level_1',\n",
    "#             'brand', 'attributes', 'price_hint', 'description', 'sku_id']\n",
    "features = ['products', 'parentProducts', 'brand', 'price_hint', 'title', 'category_id', 'id']\n",
    "fts = {}\n",
    "for i in range(len(features)):\n",
    "    fts[features[i]] = i\n",
    "df = raw_df[features].copy()\n",
    "\n",
    "\n",
    "# pre-process data\n",
    "# convert to float\n",
    "df.price_hint = df.price_hint.astype(float)\n",
    "# fill missing\n",
    "df.price_hint.fillna(df.price_hint.median(), inplace=True)\n",
    "\n",
    "\n",
    "# tokenize and stem function for feature extraction\n",
    "def tokenize_and_stem(text):\n",
    "    # load nltk's stemmer object\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word', min_df=0, max_df=0.9, tokenizer=tokenize_and_stem, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(df['title'])\n",
    "\n",
    "\n",
    "# seq = range(200, 1201, 50)\n",
    "# var_track = []\n",
    "# for i in seq:\n",
    "#     svd = TruncatedSVD(n_components=i, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "#     tfidf_rd = svd.fit_transform(tfidf_matrix)\n",
    "#     var_track.append(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# title processing\n",
    "def title_process(df):\n",
    "    # calculate tfidf matrix for title\n",
    "    tf = TfidfVectorizer(analyzer='word', min_df=0, max_df=0.9, tokenizer=tokenize_and_stem, stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(df['title'])\n",
    "    # Latent semantic analysis and re-normalization for tfidf matrix (dimension reduction)\n",
    "    svd = TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    tfidf_rd = lsa.fit_transform(tfidf_matrix)\n",
    "    return tfidf_rd\n",
    "\n",
    "\n",
    "title_mat = title_process(df)\n",
    "# col_name = ['t'+str(i) for i in range(200)]\n",
    "# df_clean = pd.concat([df, pd.DataFrame(title_mat, columns=col_name)], axis=1)\n",
    "mat = np.concatenate((df.values, title_mat), axis=1)\n",
    "\n",
    "\n",
    "# product processing using pandas df\n",
    "# def prod_process(a, b):\n",
    "#     # calculate products and parentProducts distance\n",
    "#     # either one is missing\n",
    "#     if pd.isnull(a.loc['products']) or pd.isnull(b.loc['products']):\n",
    "#         prod_dist = 0.8\n",
    "#     # same products\n",
    "#     elif a.loc['products'] == b.loc['products']:\n",
    "#         prod_dist = 0\n",
    "#     # one's product is same as another's parentProduct\n",
    "#     elif a.loc['products'] == b.loc['parentProducts'] or b.loc['products'] == a.loc['parentProducts']:\n",
    "#         prod_dist = 0.1\n",
    "#     # one's parentProduct is part of another's product\n",
    "#     elif (pd.notnull(a.loc['parentProducts']) and a.loc['parentProducts'] in b.loc['products']) or \\\n",
    "#             (pd.notnull(b.loc['parentProducts']) and b.loc['parentProducts'] in a.loc['products']):\n",
    "#         prod_dist = 0.3\n",
    "#     # one's product is part of another's product\n",
    "#     elif a.loc['products'] in b.loc['products'] or b.loc['products'] in a.loc['products']:\n",
    "#         prod_dist = 0.3\n",
    "#     else:\n",
    "#         prod_dist = 1\n",
    "#     return prod_dist\n",
    "\n",
    "\n",
    "def prod_process(a, b):\n",
    "    # calculate products and parentProducts distance\n",
    "    # either one is missing\n",
    "    if pd.isnull(a[fts['products']]) or pd.isnull(b[fts['products']]):\n",
    "        prod_dist = 0.8\n",
    "    # same products\n",
    "    elif a[fts['products']] == b[fts['products']]:\n",
    "        prod_dist = 0\n",
    "    # one's product is same as another's parentProduct\n",
    "    elif a[fts['products']] == b[fts['parentProducts']] or b[fts['products']] == a[fts['parentProducts']]:\n",
    "        prod_dist = 0.1\n",
    "    # one's parentProduct is part of another's product\n",
    "    elif (pd.notnull(a[fts['parentProducts']]) and a[fts['parentProducts']] in b[fts['products']]) or \\\n",
    "            (pd.notnull(b[fts['parentProducts']]) and b[fts['parentProducts']] in a[fts['products']]):\n",
    "        prod_dist = 0.3\n",
    "    # one's product is part of another's product\n",
    "    elif a[fts['products']] in b[fts['products']] or b[fts['products']] in a[fts['products']]:\n",
    "        prod_dist = 0.3\n",
    "    else:\n",
    "        prod_dist = 1\n",
    "    return prod_dist\n",
    "\n",
    "\n",
    "# price processing\n",
    "def price_process(a, b):\n",
    "    i = a[fts['price_hint']]\n",
    "    j = b[fts['price_hint']]\n",
    "    return np.abs(i - j)/(i + j)\n",
    "\n",
    "\n",
    "# brand processing\n",
    "def brand_process(a, b):\n",
    "    if a[fts['brand']] == b[fts['brand']]:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "# calculate weighted distance\n",
    "def mixed_dist(a, b, prod_wt=0.5, brand_wt=0.2, title_wt=0.2, price_wt=0.1):\n",
    "    # calculate title_dist\n",
    "    title_dist = title_only(a, b)\n",
    "    # calculate prod_dist\n",
    "    prod_dist = prod_process(a, b)\n",
    "    # calculate price_dist\n",
    "    price_dist = price_process(a, b)\n",
    "    # calculate brand_dist\n",
    "    brand_dist = brand_process(a, b)\n",
    "    distance = np.dot([prod_wt, brand_wt, title_wt, price_wt],\n",
    "                  [prod_dist, brand_dist, title_dist, price_dist])\n",
    "    return distance\n",
    "\n",
    "\n",
    "# calculate title distance only\n",
    "def title_only(a, b):\n",
    "    title_dist = (1 - np.dot(a[7:], b[7:])) * 0.5\n",
    "    return title_dist\n",
    "\n",
    "\n",
    "# generate top-k recommendation list given an index\n",
    "def query(ind, k=30, dist=mixed_dist, data=mat):\n",
    "    idx = data[ind, fts['id']]\n",
    "    # indices = np.arange(len(data))\n",
    "    # temp_data = data[indices!=ind, :]\n",
    "    # temp_data = np.delete(data, ind, 0)\n",
    "    # dist_mat = np.apply_along_axis(dist, axis=1, arr=temp_data, b=data[ind, :])\n",
    "    dist_mat = np.apply_along_axis(dist, axis=1, arr=data, b=data[ind, :])\n",
    "    temp_ind = np.argpartition(dist_mat, k+1)[:k+1]\n",
    "    top_ind = temp_ind[np.argsort(dist_mat[temp_ind])]\n",
    "    if ind in top_ind:\n",
    "        top_ind = top_ind[top_ind != ind]\n",
    "    else:\n",
    "        top_ind = top_ind[:k]\n",
    "    top_score = dist_mat[top_ind]\n",
    "    # top_idy = temp_data[top_ind, fts['id']]\n",
    "    top_idy = data[top_ind, fts['id']]\n",
    "    result = []\n",
    "    for idy, value in zip(top_idy, top_score):\n",
    "        pair = dict()\n",
    "        pair['idX'] = idx\n",
    "        pair['idY'] = idy\n",
    "        pair['method'] = 'content_based_v1'\n",
    "        pair['score'] = value\n",
    "        result.append(pair)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random sample indices\n",
    "sample = randint(0, len(data), 1000)\n",
    "\n",
    "\n",
    "# parallel computing\n",
    "def parallel(func, chunk=sample, p=6):\n",
    "    pool = Pool(processes=p)\n",
    "    result = pool.map(func, chunk)\n",
    "    flat_result = [sku for rs_list in result for sku in rs_list]\n",
    "    return flat_result\n",
    "\n",
    "\n",
    "# generate overall recommendation pair list\n",
    "start = time.time()\n",
    "rs_output = parallel(query, sample, 6)\n",
    "print(time.time() - start)\n",
    "\n",
    "\n",
    "# def test(id, k=30, dist=mixed_dist, data=mat):\n",
    "#     temp_data = np.delete(data, id, 0)\n",
    "#     dist_mat = np.apply_along_axis(dist, axis=1, arr=temp_data, b=mat[id, :])\n",
    "#     idx = np.argpartition(dist_mat, k)[:k]\n",
    "#     top_idx = idx[np.argsort(dist_mat[idx])]\n",
    "#     result = temp_data[top_idx, :]\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '960f5a1190984ee99532dfa6b9656d1b',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.440328308616877},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '1af68a88a1404b109809c328c29c7ff9',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.4542496367784164},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '857bda69303a4991aa33b183b1fe1de4',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.46726903544871784},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '51451510d0e14a2eb87f3086e30565ba',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.48387953791125582},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '2cd108612ac64bcaa46c1befc334ed44',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.4919299629244856},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': 'ee0567593c1e47adaff11c73234b8f76',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.49605034797394665},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '7ac0055adc1641b08491bc585bc82bf9',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.49655370019632672},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '9721ea3afb8146d988a064c73e346e21',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.49657700714029707},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': '479659b5c87643e6b6155346129af4c0',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.49797874433502715},\n",
       " {'idX': '6a0617444b5c4b37b9e7cd9191de63bc',\n",
       "  'idY': 'd9606a0f2d4045afb9c06b98a76fa478',\n",
       "  'method': 'content_based_v1',\n",
       "  'score': 0.50215253638509871}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
